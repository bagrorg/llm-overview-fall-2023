# Эмбеддинги
В постоянном потоке текстовых данных, с которыми сталкиваются системы обработки естественного языка и языковые модели, возникает важный вопрос: как эффективно представить слова и фразы так, чтобы алгоритмы могли понимать их смысл и контекст? Эмбеддинги, или встроенные представления, вступают в игру как ключевой инструмент в решении этой задачи.

Эмбеддинги представляют собой механизм, позволяющий преобразовать слова в векторы чисел, сохраняя при этом их семантические отношения и контекстуальные особенности. На практике это означает, что слова, близкие по смыслу, имеют близкие векторные представления.

В этой части будет рассмотрено несколько вариантов того, как можно строить эти представления.

## One-hot encoding
One-Hot Encoding предоставляет уникальный способ представления категориальных данных в виде бинарных векторов. При использовании этого метода каждое слово или фраза превращаются в вектор с единственным взведенным битом, соответствующем своей категории, а все остальные биты устанавливаются в ноль. В случае работы с естественным языком `i`-тому слову в словаре соответствует вектор с взведенным битом на `i`-той позиции.

Подход довольно прост в использовании, но крайне неэффективен: 

1. Из-за своего устройства у эмбеддингов слишком большая размерность, которая ведет к закономерным последствиям, ухудшающим производительность (повышается использование памяти и накладные расходы на вычисления)

2. Данный подход не сохраняет никакую информацию об использовании слов в тексте: статистику их позиций, статистику их использований и т.д.

## Частотный эмбеддинг
Предположим, что у нас есть набор документов $D$. Для того чтобы начать учитывать важность слова основываясь на его статистике вхождения в текст можно воспользоваться метрикой TF-IDF. Она состоит из двух частей: TF и IDF -- двух чисел, которые затем перемножаются.

**TF** (term frequency) -- частота слова $t$ в рамках одного документа $d$.
$$TF(t, d) = \frac{n_t}{\sum_k n_k}$$

**IDF** (inverse document frequency) -- обратная частота, с которой некоторое слово встречается в документах коллекции.
$$IDF(t, D) = \log \frac{|D|}{|\{d_i \in D | t \in d_i|\}}$$

Таким образом большой вес получают слова с высокой частотой в пределах данного документа $d$, но менее распространенные в рамках все корпуса.

При этом TF-IDF обладает определенными недостатками: никак не учитывается порядок и семантика слова, что может внести определенный шум.

## Представление слов в векторном пространстве
Тут будет разговор основанный на `Efficient estimation of word representations in vector space, 2013`. 

## Токенизация
Тут будет разговор про базу токенизации

### Subword Tokenization
Тут будет пару слов про токенизацию частей слов

### Byte Pair Encoding
Тут будет разговор про BPE

### Word Piece
Тут будет разговор про WordPiece.

### Unigram Language Model
ULM подход позволяет сократить размер словаря и учесть разные возможные разбиения на токены.

Предположим есть заранее заданный словарь $V$ (при этом содержащий также все однобуквенные токены, чтобы избежать out of vocabulary) и набор документов $D$. Также делается предположение, что все слова независимо появляются в тексте, т.е. для входной последовательности $x = (x_1, ..., x_n)$
верно, что
$$ P(x) = \prod p(x_i) \\ \sum_{x \in V} p(x) = 1 $$

При этом для входной последовательности $x$ определяется наиболее вероятное разбиение $x^*$ как 
$$ x^* = argmax_{x' \in S(x)} P(x') $$

где $S(x)$ это всевозможные разбиения $x$. Это можно сделать с помощью [алгоритма Витерби](https://ieeexplore.ieee.org/document/1054010).

Мы хотим проредить наш словарь, оставив только действительно нужные токены. Для этого введем функцию правдоподобия для нашего $D$

$$ \sum_{d \in D} \log P(d) = \sum_{d \in D} \log \big( \sum_{x \in S(d)} P(x) \big) $$

Таким образом строится следующий алгоритм:
1. С помощью EM-алгоритма предподсчитывается $p(x)$
2. Для каждого токена считается его $loss$ -- значения правдоподобия, если бы его удалили из словаря
3. Сортируем токены по $loss$ и оставляем топ-$y$ (например 80%)

При этом важно оставлять односимвольные токены, чтобы избегать OOV.
